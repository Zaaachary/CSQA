{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('torch')",
   "metadata": {
    "interpreter": {
     "hash": "00d224a08ec1428a6c57a1d9b2b26e1b504667d300aa8f79539c072313c89ab9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertConfig, AlbertTokenizer\n",
    "from transformers import AlbertModel as tfms_AlbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.AlbertModel import AlbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2,  114,   57,   21, 1289,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(r'D:\\CODE\\Python\\Transformers-Models\\albert-base-v2')\n",
    "target = ['just have a test']\n",
    "feature_dict = tokenizer.batch_encode_plus(target, return_tensors='pt')\n",
    "feature_dict"
   ]
  },
  {
   "source": [
    "### origin albert base"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_albert = tfms_AlbertModel.from_pretrained(r'D:\\CODE\\Python\\Transformers-Models\\albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_output = origin_albert(**feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 1.2229,  0.7311,  0.5204,  ..., -0.1884,  0.4021,  0.2687],\n",
       "         [ 0.1540,  0.5329,  0.7220,  ..., -0.5679, -0.1582,  0.1250],\n",
       "         [ 0.0161, -0.2441,  0.5613,  ...,  1.0106,  0.7557, -1.1506],\n",
       "         [ 0.7950,  0.5904,  2.4772,  ...,  0.1583,  1.2289, -0.1720],\n",
       "         [ 0.8335, -0.4584, -0.2803,  ...,  0.4956,  0.5529, -1.5881],\n",
       "         [ 0.0646,  0.1395, -0.0524,  ..., -0.0824,  0.1352,  0.2109]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "origin_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert0 = AlbertModel.from_pretrained(r'D:\\CODE\\Python\\Transformers-Models\\albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_0 = albert0(**feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 1.2229,  0.7311,  0.5204,  ..., -0.1884,  0.4021,  0.2687],\n",
       "         [ 0.1540,  0.5329,  0.7220,  ..., -0.5679, -0.1582,  0.1250],\n",
       "         [ 0.0161, -0.2441,  0.5613,  ...,  1.0106,  0.7557, -1.1506],\n",
       "         [ 0.7950,  0.5904,  2.4772,  ...,  0.1583,  1.2289, -0.1720],\n",
       "         [ 0.8335, -0.4584, -0.2803,  ...,  0.4956,  0.5529, -1.5881],\n",
       "         [ 0.0646,  0.1395, -0.0524,  ..., -0.0824,  0.1352,  0.2109]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "output_0[0]"
   ]
  },
  {
   "source": [
    "### half of albert base"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = AlbertConfig(\n",
    "    hidden_size=768,\n",
    "    intermediate_size=3072,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    without_embedding=False\n",
    "    )\n",
    "albert1 = AlbertModel.from_pretrained((r'D:\\CODE\\Python\\Transformers-Models\\albert-base-v2'), config=config1)\n",
    "\n",
    "config2 = AlbertConfig(\n",
    "    hidden_size=768,\n",
    "    intermediate_size=3072,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    without_embedding=True\n",
    ")\n",
    "\n",
    "albert2 = AlbertModel.from_pretrained((r'D:\\CODE\\Python\\Transformers-Models\\albert-base-v2'), config=config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = albert1(**feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.3021, -0.2515,  2.3462,  ...,  0.5683,  0.4035, -0.1834],\n",
       "         [-1.2193,  0.4084, -1.5469,  ..., -1.2692, -0.5502,  0.9160],\n",
       "         [-0.6181,  2.0706,  0.4986,  ..., -0.8264, -0.2388, -0.4955],\n",
       "         [ 0.1945,  0.0731,  1.5474,  ..., -0.8568,  1.2589, -0.0540],\n",
       "         [ 0.4074, -0.9898, -0.0415,  ...,  0.0398,  0.9352,  0.1003],\n",
       "         [-0.0242, -0.0186, -0.0773,  ...,  0.0784, -0.0342,  0.0041]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "output1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = albert2(inputs_embeds = output1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 1.2229,  0.7311,  0.5204,  ..., -0.1884,  0.4021,  0.2687],\n",
       "         [ 0.1540,  0.5329,  0.7220,  ..., -0.5679, -0.1582,  0.1250],\n",
       "         [ 0.0161, -0.2441,  0.5613,  ...,  1.0106,  0.7557, -1.1506],\n",
       "         [ 0.7950,  0.5904,  2.4772,  ...,  0.1583,  1.2289, -0.1720],\n",
       "         [ 0.8335, -0.4584, -0.2803,  ...,  0.4956,  0.5529, -1.5881],\n",
       "         [ 0.0646,  0.1395, -0.0524,  ..., -0.0824,  0.1352,  0.2109]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "output2[0]"
   ]
  }
 ]
}