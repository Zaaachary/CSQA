{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('CSQA')",
   "metadata": {
    "interpreter": {
     "hash": "90f16fc473da99dae7f28739a8f74963c792de6ae40ab2f78d57652613f3a601"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d:\\CODE\\Commonsense\\CSQA_dev\\CODE\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AlbertPreTrainedModel, AlbertTokenizer, AlbertConfig\n",
    "\n",
    "from model.AlbertModel import AlbertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertBurger(nn.Module):\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "\n",
    "        super(AlbertBurger, self).__init__()\n",
    "\n",
    "        albert1_layers = kwargs['albert1_layers']\n",
    "\n",
    "        self.config1 = deepcopy(config)\n",
    "        self.config1.num_hidden_layers = albert1_layers\n",
    "        self.config2 = deepcopy(config)\n",
    "        self.config2.num_hidden_layers = config.num_hidden_layers - albert1_layers\n",
    "        self.config2.without_embedding = True\n",
    "\n",
    "        self.albert1 = AlbertModel(self.config1)\n",
    "        self.albert2 = AlbertModel(self.config2)\n",
    "\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "        \n",
    "        outputs = self.albert1(input_ids, attention_mask, token_type_ids)\n",
    "        hidden_state_1 = outputs.last_hidden_state\n",
    "        outputs = self.albert2(inputs_embeds=hidden_state_1)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path_or_name, **kwargs):\n",
    "\n",
    "        config = AlbertConfig()\n",
    "        config.without_embedding = False\n",
    "        if \"xxlarge\" in model_path_or_name:\n",
    "            config.hidden_size = 4096\n",
    "            config.intermediate_size = 16384\n",
    "            config.num_attention_heads = 64\n",
    "            config.num_hidden_layers = 12\n",
    "        elif \"xlarge\" in model_path_or_name:\n",
    "            config.hidden_size = 2048\n",
    "            config.intermediate_size = 8192\n",
    "            config.num_attention_heads = 16\n",
    "            config.num_hidden_layers = 24\n",
    "        elif \"large\" in model_path_or_name:\n",
    "            config.hidden_size = 1024\n",
    "            config.intermediate_size = 4096\n",
    "            config.num_attention_heads = 16\n",
    "            config.num_hidden_layers = 24\n",
    "        elif \"base\" in model_path_or_name:\n",
    "            config.hidden_size = 768\n",
    "            config.intermediate_size = 3072\n",
    "            config.num_attention_heads = 12\n",
    "            config.num_hidden_layers = 12\n",
    "\n",
    "        model = cls(config, **kwargs)\n",
    "        model.albert1 = model.albert1.from_pretrained(model_path_or_name, config=model.config1)\n",
    "        model.albert2 = model.albert2.from_pretrained(model_path_or_name, config=model.config2)\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at D:\\CODE\\Python\\Transformers-Models\\albert-base-v2 were not used when initializing AlbertModel: ['albert.embeddings.word_embeddings.weight', 'albert.embeddings.position_embeddings.weight', 'albert.embeddings.token_type_embeddings.weight', 'albert.embeddings.LayerNorm.weight', 'albert.embeddings.LayerNorm.bias']\n- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model = AlbertBurger.from_pretrained(r'D:\\CODE\\Python\\Transformers-Models\\albert-base-v2', albert1_run=6)\n",
    "kwargs = {'albert1_layers': 6}\n",
    "model = AlbertBurger.from_pretrained(r'D:\\CODE\\Python\\Transformers-Models\\albert-base-v2', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2,  114,   57,   21, 1289,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(r'D:\\CODE\\Python\\Transformers-Models\\albert-base-v2')\n",
    "feature_dict = tokenizer.batch_encode_plus(['just have a test',], return_tensors='pt')\n",
    "feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 1.2229,  0.7311,  0.5204,  ..., -0.1884,  0.4021,  0.2687],\n",
       "         [ 0.1540,  0.5329,  0.7220,  ..., -0.5679, -0.1582,  0.1250],\n",
       "         [ 0.0161, -0.2441,  0.5613,  ...,  1.0106,  0.7557, -1.1507],\n",
       "         [ 0.7950,  0.5904,  2.4772,  ...,  0.1583,  1.2289, -0.1720],\n",
       "         [ 0.8335, -0.4584, -0.2803,  ...,  0.4956,  0.5529, -1.5881],\n",
       "         [ 0.0646,  0.1395, -0.0524,  ..., -0.0824,  0.1352,  0.2109]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "model(**feature_dict)"
   ]
  }
 ]
}